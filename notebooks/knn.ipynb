{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis con KNN\n",
    "## Clasificador en C++ ðŸ’ªðŸ’ª\n",
    "Vamos a probar a nuestro bichito\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir los path al ejecutable de python 3.6 y sus librerÃ­as,\n",
    "de acuerdo al virtual env que estÃ©n corriendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜buildâ€™: File exists\n",
      "-- The C compiler identification is GNU 8.3.0\n",
      "-- The CXX compiler identification is GNU 8.3.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "Release mode\n",
      "-- Found PythonInterp: /home/tdelgado/anaconda3/bin/python (found version \"3.7.3\") \n",
      "-- Found PythonLibs: /home/tdelgado/anaconda3/lib/libpython3.7m.so\n",
      "-- pybind11 v2.3.dev0\n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- LTO enabled\n",
      "CMAKE_INSTALL_PREFIX=/home/tdelgado/Desktop/Tp2Metodos\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/tdelgado/Desktop/Tp2Metodos/build\n",
      "\u001b[35m\u001b[1mScanning dependencies of target tp2_pybind\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object CMakeFiles/tp2_pybind.dir/src/main_pybind.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/tp2_pybind.dir/src/knn.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/tp2_pybind.dir/src/pca.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/tp2_pybind.dir/src/eigen.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CXX executable tp2_pybind\u001b[0m\n",
      "[ 33%] Built target tp2_pybind\n",
      "\u001b[35m\u001b[1mScanning dependencies of target tp2\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/tp2.dir/src/main.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/tp2.dir/src/knn.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/tp2.dir/src/pca.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/tp2.dir/src/eigen.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable tp2\u001b[0m\n",
      "[ 66%] Built target tp2\n",
      "\u001b[35m\u001b[1mScanning dependencies of target sentiment\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/sentiment.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/knn.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/pca.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/eigen.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module sentiment.cpython-37m-x86_64-linux-gnu.so\u001b[0m\n",
      "[100%] Built target sentiment\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"Release\"\n",
      "-- Installing: /home/tdelgado/Desktop/Tp2Metodos/notebooks/sentiment.cpython-37m-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "!cd .. && git submodule init\n",
    "!cd .. && git submodule update\n",
    "!cd .. && mkdir build\n",
    "!cd ../build/ && rm -rf *\n",
    "!cd ../build && cmake \\\n",
    "  -DPYTHON_EXECUTABLE=\"$(which python)\" \\\n",
    "  -DCMAKE_BUILD_TYPE=Release ..\n",
    "!cd ../build && make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tdelgado/Desktop/Tp2Metodos/notebooks\n",
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "# Verifico la correcta instalaciÃ³n. Si no falla el import estÃ¡ OK\n",
    "!pwd\n",
    "!python --version\n",
    "import sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAR_IMDB_LARGE = False\n",
    "SIZE_TRAIN = 5000\n",
    "SIZE_TEST = 500\n",
    "DATA_SIZE = 3000 #CANTIDAD DE REVIEWS A LEER DEL DATASET, LO PONGO PORQUE 50k PUEDE TARDAR\n",
    "                 #LA CANTIDAD TOTAL ES DATA_SIZE*4 PORQUE TOMA TEST TRAIN POS Y NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "imdb_small.csv\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.494240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  12500.000000\n",
       "mean       0.494240\n",
       "std        0.499987\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "!cd ../data && tar -xvf *.tar.gz\n",
    "\n",
    "if not USAR_IMDB_LARGE:\n",
    "    df = pd.read_csv(\"../data/imdb_small.csv\", index_col=0) \n",
    "    df = df[['review','label']]\n",
    "    df['label'] = (df['label'] == 'pos').astype('int')\n",
    "else:\n",
    "    folder = '../data/aclImdb'\n",
    "    labels = {'pos': 1, 'neg': 0}\n",
    "    df = pd.DataFrame()\n",
    "    for f in ('test', 'train'):    \n",
    "        for l in ('pos', 'neg'):\n",
    "            path = os.path.join(folder, f, l)\n",
    "            c = 0\n",
    "            for file in os.listdir (path) :\n",
    "                with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
    "                    txt = infile.read()\n",
    "                df = df.append([[txt, labels[l]]],ignore_index=True)\n",
    "                c+=1\n",
    "                if c == DATA_SIZE:\n",
    "                    break\n",
    "    df.columns = ['review', 'label']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de instancias de entrenamiento = 5000\n",
      "Cantidad de instancias de test = 500\n",
      "Class balance : 0.4902 pos 0.5098 neg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(df['review'].tolist())\n",
    "y = np.array(df['label'].tolist())\n",
    "np.random.shuffle(X)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "X_train = X[:SIZE_TRAIN]\n",
    "y_train = y[:SIZE_TRAIN]\n",
    "X_test = X[SIZE_TRAIN:SIZE_TRAIN+SIZE_TEST]\n",
    "y_test = y[SIZE_TRAIN:SIZE_TRAIN+SIZE_TEST]\n",
    "\n",
    "print(\"Cantidad de instancias de entrenamiento = {}\".format(len(X_train)))\n",
    "print(\"Cantidad de instancias de test = {}\".format(len(X_test)))\n",
    "\n",
    "print(\"Class balance : {} pos {} neg\".format(\n",
    "    y_train.sum() / y_train.shape[0], \n",
    "    (y_train.shape[0] - y_train.sum()) / y_train.shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,3), use_idf = True, smooth_idf = True, max_df = 0.9, min_df = 0.01)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3884)\n"
     ]
    }
   ],
   "source": [
    "ALFA = 500\n",
    "print(X_train.shape)\n",
    "pca_transformer = sentiment.PCA(ALFA)\n",
    "pca_transformer.fit(X_train.todense())\n",
    "X_train = pca_transformer.transform(X_train)\n",
    "X_test = pca_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "\n",
    "clf = sentiment.KNNClassifier(K)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
